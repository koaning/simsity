{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"simsity Simsity is a Super Simple Similarities Service[tm]. It's all about building a neighborhood. Literally! This project contains simple tools to help in similarity retreival scenarios by making a convientwrapper around encoding strategies as well as nearest neighbor approaches. Typical usecases include early stage bulk labelling and duplication discovery. Installation \u00b6 You can install simsity via pip . python -m pip install simsity Getting Started \u00b6 If you'd like to get started, we recommend starting here . Related Projects \u00b6 This tool becomes even more powerful when you combine it with existing tools. In particular this library was designed to work well with: scikit-learn for general encoding tools and pipelines whatlies for encoding tools on text data dirty_cat for encoding tools on dirty categorical data","title":"Home"},{"location":"#installation","text":"You can install simsity via pip . python -m pip install simsity","title":"Installation"},{"location":"#getting-started","text":"If you'd like to get started, we recommend starting here .","title":"Getting Started"},{"location":"#related-projects","text":"This tool becomes even more powerful when you combine it with existing tools. In particular this library was designed to work well with: scikit-learn for general encoding tools and pipelines whatlies for encoding tools on text data dirty_cat for encoding tools on dirty categorical data","title":"Related Projects"},{"location":"api/datasets/","text":"from simsity.datasets import fetch_clinc \u00b6 Loads the clinc conversational intents data. Usage: from simsity.datasets import fetch_clinc fetch_clinc () from simsity.datasets import fetch_voters \u00b6 Loads the voters dataset. Usage: from simsity.datasets import fetch_voters fetch_voters ()","title":"simsity.datasets"},{"location":"api/datasets/#from-simsitydatasets-import-fetch_clinc","text":"Loads the clinc conversational intents data. Usage: from simsity.datasets import fetch_clinc fetch_clinc ()","title":"from simsity.datasets import fetch_clinc"},{"location":"api/datasets/#from-simsitydatasets-import-fetch_voters","text":"Loads the voters dataset. Usage: from simsity.datasets import fetch_voters fetch_voters ()","title":"from simsity.datasets import fetch_voters"},{"location":"api/indexer/","text":"PyNNDescentIndexer \u00b6 An indexer based on PyNNDescent. Parameters Name Type Description Default metric The metric to use for the index. 'euclidean' n_neighbors The number of neighbors to use for the index. 10 random_state The random state to use for the index. 42 n_jobs The number of parallel jobs to run for neighbors index construction. 1 index ( self , data ) \u00b6 Show source code in indexer/pynn.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def index ( self , data ): \"\"\" Index the given data. Arguments: data: The data to index. \"\"\" self . model = NNDescent ( data , metric = self . metric , n_neighbors = self . n_neighbors , random_state = self . random_state , n_jobs = self . n_jobs , ) self . model . prepare () Index the given data. Parameters Name Type Description Default data The data to index. required load ( path ) (classmethod) \u00b6 Show source code in indexer/pynn.py 81 82 83 84 85 86 87 88 89 @classmethod def load ( cls , path ) -> \"Indexer\" : \"\"\" Load the indexer in a path. Arguments: path: string or pathlib.Path to folder. \"\"\" return load ( pathlib . Path ( path ) / \"indexer.joblib\" ) Load the indexer in a path. Parameters Name Type Description Default path string or pathlib.Path to folder. required query ( self , query , n_neighbors = 1 ) \u00b6 Show source code in indexer/pynn.py 47 48 49 50 51 52 53 54 55 56 57 58 def query ( self , query , n_neighbors = 1 ): \"\"\" Query the index. Arguments: query: The query to query the index with. n_neighbors: The number of neighbors to return. \"\"\" if not self . model : raise RuntimeError ( \"Index not yet built.\" ) idx , dist = self . model . query ( query , n_neighbors ) return list ( idx [ 0 ]), list ( dist [ 0 ]) Query the index. Parameters Name Type Description Default query The query to query the index with. required n_neighbors The number of neighbors to return. 1 save ( self , path ) \u00b6 Show source code in indexer/pynn.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def save ( self , path ) -> None : \"\"\" Save the indexer in a path. Arguments: path: string or pathlib.Path to folder. \"\"\" # Save the index dump ( self , pathlib . Path ( path ) / \"indexer.joblib\" ) # Save the metadata so that we have the parameters on load. metadata_path = Path ( path ) / \"metadata.json\" metadata = json . loads ( metadata_path . read_text ()) metadata [ \"pynn\" ] = dict ( metric = self . metric , n_neighbors = self . n_neighbors , random_state = self . random_state , n_jobs = self . n_jobs , ) metadata_path . write_text ( json . dumps ( metadata )) Save the indexer in a path. Parameters Name Type Description Default path string or pathlib.Path to folder. required AnnoyIndexer \u00b6 An indexer based on Annoy. Note! Annoy does not support sparse data. Parameters Name Type Description Default metric The metric to use for the index. Can be angular , euclidean , manning , manhattan or dot . 'euclidean' n_trees The number of trees to build. 10 n_jobs Degree of parallism used while training. 1 index ( self , data ) \u00b6 Show source code in indexer/annoy.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def index ( self , data ): \"\"\" Index the given data. Arguments: data: The data to index. \"\"\" if isinstance ( data , spmatrix ): raise ValueError ( \"Annoy index does not support sparse matrices.\" ) self . feature_size = data . shape [ 1 ] self . model = AnnoyIndex ( self . feature_size , self . metric ) for i in range ( data . shape [ 0 ]): if isinstance ( data , pd . DataFrame ): self . model . add_item ( i , data . iloc [ i ] . values ) else : self . model . add_item ( i , data [ i ]) self . model . build ( self . n_trees , n_jobs = self . n_jobs ) Index the given data. Parameters Name Type Description Default data The data to index. required load ( path ) (classmethod) \u00b6 Show source code in indexer/annoy.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 @classmethod def load ( self , path ) -> \"Indexer\" : \"\"\" Load the indexer in a path. Arguments: path: string or pathlib.Path to folder. \"\"\" # Load metadata as well metadata_path = Path ( path ) / \"metadata.json\" metadata = json . loads ( metadata_path . read_text ())[ \"annoy\" ] # Prepare keyword arguments keyword_args = dict ( metric = metadata [ \"metric\" ], n_trees = metadata [ \"n_trees\" ], random_state = metadata [ \"random_state\" ], n_jobs = metadata [ \"n_jobs\" ], ) # Construct an empty, but configured index, before loading from disk index = AnnoyIndexer ( ** keyword_args ) index . model = AnnoyIndex ( metadata [ \"feature_size\" ], keyword_args [ \"metric\" ]) index . model . load ( str ( Path ( path ) / \"index.ann\" )) return index Load the indexer in a path. Parameters Name Type Description Default path string or pathlib.Path to folder. required query ( self , query , n_neighbors = 1 ) \u00b6 Show source code in indexer/annoy.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def query ( self , query , n_neighbors = 1 ): \"\"\" Query the index. Arguments: query: The query to query the index with. n_neighbors: The number of neighbors to return. \"\"\" if not self . model : raise RuntimeError ( \"Index not yet built.\" ) if isinstance ( query , pd . DataFrame ): query = query . iloc [ 0 ] . values idx , dist = self . model . get_nns_by_vector ( query , n = n_neighbors , include_distances = True ) return idx , dist Query the index. Parameters Name Type Description Default query The query to query the index with. required n_neighbors The number of neighbors to return. 1 save ( self , path ) \u00b6 Show source code in indexer/annoy.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def save ( self , path ) -> None : \"\"\" Save the indexer in a path. Arguments: path: string or pathlib.Path to folder. \"\"\" # Save the index self . model . save ( str ( Path ( path ) / \"index.ann\" )) # Save the metadata so that we have the parameters on load. metadata_path = Path ( path ) / \"metadata.json\" metadata = json . loads ( metadata_path . read_text ()) metadata [ \"annoy\" ] = dict ( metric = self . metric , n_trees = self . n_trees , random_state = self . random_state , n_jobs = self . n_jobs , feature_size = self . feature_size , ) metadata_path . write_text ( json . dumps ( metadata )) Save the indexer in a path. Parameters Name Type Description Default path string or pathlib.Path to folder. required","title":"simsity.indexer"},{"location":"api/indexer/#pynndescentindexer","text":"An indexer based on PyNNDescent. Parameters Name Type Description Default metric The metric to use for the index. 'euclidean' n_neighbors The number of neighbors to use for the index. 10 random_state The random state to use for the index. 42 n_jobs The number of parallel jobs to run for neighbors index construction. 1","title":"PyNNDescentIndexer"},{"location":"api/indexer/#simsity.indexer.pynn.PyNNDescentIndexer.index","text":"Show source code in indexer/pynn.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def index ( self , data ): \"\"\" Index the given data. Arguments: data: The data to index. \"\"\" self . model = NNDescent ( data , metric = self . metric , n_neighbors = self . n_neighbors , random_state = self . random_state , n_jobs = self . n_jobs , ) self . model . prepare () Index the given data. Parameters Name Type Description Default data The data to index. required","title":"index()"},{"location":"api/indexer/#simsity.indexer.pynn.PyNNDescentIndexer.load","text":"Show source code in indexer/pynn.py 81 82 83 84 85 86 87 88 89 @classmethod def load ( cls , path ) -> \"Indexer\" : \"\"\" Load the indexer in a path. Arguments: path: string or pathlib.Path to folder. \"\"\" return load ( pathlib . Path ( path ) / \"indexer.joblib\" ) Load the indexer in a path. Parameters Name Type Description Default path string or pathlib.Path to folder. required","title":"load()"},{"location":"api/indexer/#simsity.indexer.pynn.PyNNDescentIndexer.query","text":"Show source code in indexer/pynn.py 47 48 49 50 51 52 53 54 55 56 57 58 def query ( self , query , n_neighbors = 1 ): \"\"\" Query the index. Arguments: query: The query to query the index with. n_neighbors: The number of neighbors to return. \"\"\" if not self . model : raise RuntimeError ( \"Index not yet built.\" ) idx , dist = self . model . query ( query , n_neighbors ) return list ( idx [ 0 ]), list ( dist [ 0 ]) Query the index. Parameters Name Type Description Default query The query to query the index with. required n_neighbors The number of neighbors to return. 1","title":"query()"},{"location":"api/indexer/#simsity.indexer.pynn.PyNNDescentIndexer.save","text":"Show source code in indexer/pynn.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def save ( self , path ) -> None : \"\"\" Save the indexer in a path. Arguments: path: string or pathlib.Path to folder. \"\"\" # Save the index dump ( self , pathlib . Path ( path ) / \"indexer.joblib\" ) # Save the metadata so that we have the parameters on load. metadata_path = Path ( path ) / \"metadata.json\" metadata = json . loads ( metadata_path . read_text ()) metadata [ \"pynn\" ] = dict ( metric = self . metric , n_neighbors = self . n_neighbors , random_state = self . random_state , n_jobs = self . n_jobs , ) metadata_path . write_text ( json . dumps ( metadata )) Save the indexer in a path. Parameters Name Type Description Default path string or pathlib.Path to folder. required","title":"save()"},{"location":"api/indexer/#annoyindexer","text":"An indexer based on Annoy. Note! Annoy does not support sparse data. Parameters Name Type Description Default metric The metric to use for the index. Can be angular , euclidean , manning , manhattan or dot . 'euclidean' n_trees The number of trees to build. 10 n_jobs Degree of parallism used while training. 1","title":"AnnoyIndexer"},{"location":"api/indexer/#simsity.indexer.annoy.AnnoyIndexer.index","text":"Show source code in indexer/annoy.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def index ( self , data ): \"\"\" Index the given data. Arguments: data: The data to index. \"\"\" if isinstance ( data , spmatrix ): raise ValueError ( \"Annoy index does not support sparse matrices.\" ) self . feature_size = data . shape [ 1 ] self . model = AnnoyIndex ( self . feature_size , self . metric ) for i in range ( data . shape [ 0 ]): if isinstance ( data , pd . DataFrame ): self . model . add_item ( i , data . iloc [ i ] . values ) else : self . model . add_item ( i , data [ i ]) self . model . build ( self . n_trees , n_jobs = self . n_jobs ) Index the given data. Parameters Name Type Description Default data The data to index. required","title":"index()"},{"location":"api/indexer/#simsity.indexer.annoy.AnnoyIndexer.load","text":"Show source code in indexer/annoy.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 @classmethod def load ( self , path ) -> \"Indexer\" : \"\"\" Load the indexer in a path. Arguments: path: string or pathlib.Path to folder. \"\"\" # Load metadata as well metadata_path = Path ( path ) / \"metadata.json\" metadata = json . loads ( metadata_path . read_text ())[ \"annoy\" ] # Prepare keyword arguments keyword_args = dict ( metric = metadata [ \"metric\" ], n_trees = metadata [ \"n_trees\" ], random_state = metadata [ \"random_state\" ], n_jobs = metadata [ \"n_jobs\" ], ) # Construct an empty, but configured index, before loading from disk index = AnnoyIndexer ( ** keyword_args ) index . model = AnnoyIndex ( metadata [ \"feature_size\" ], keyword_args [ \"metric\" ]) index . model . load ( str ( Path ( path ) / \"index.ann\" )) return index Load the indexer in a path. Parameters Name Type Description Default path string or pathlib.Path to folder. required","title":"load()"},{"location":"api/indexer/#simsity.indexer.annoy.AnnoyIndexer.query","text":"Show source code in indexer/annoy.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def query ( self , query , n_neighbors = 1 ): \"\"\" Query the index. Arguments: query: The query to query the index with. n_neighbors: The number of neighbors to return. \"\"\" if not self . model : raise RuntimeError ( \"Index not yet built.\" ) if isinstance ( query , pd . DataFrame ): query = query . iloc [ 0 ] . values idx , dist = self . model . get_nns_by_vector ( query , n = n_neighbors , include_distances = True ) return idx , dist Query the index. Parameters Name Type Description Default query The query to query the index with. required n_neighbors The number of neighbors to return. 1","title":"query()"},{"location":"api/indexer/#simsity.indexer.annoy.AnnoyIndexer.save","text":"Show source code in indexer/annoy.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def save ( self , path ) -> None : \"\"\" Save the indexer in a path. Arguments: path: string or pathlib.Path to folder. \"\"\" # Save the index self . model . save ( str ( Path ( path ) / \"index.ann\" )) # Save the metadata so that we have the parameters on load. metadata_path = Path ( path ) / \"metadata.json\" metadata = json . loads ( metadata_path . read_text ()) metadata [ \"annoy\" ] = dict ( metric = self . metric , n_trees = self . n_trees , random_state = self . random_state , n_jobs = self . n_jobs , feature_size = self . feature_size , ) metadata_path . write_text ( json . dumps ( metadata )) Save the indexer in a path. Parameters Name Type Description Default path string or pathlib.Path to folder. required","title":"save()"},{"location":"api/preprocessing/","text":"Identity \u00b6 Encoder/Transformer that keeps data as-is. fit ( self , X , y ) \u00b6 Show source code in preprocessing/_identity.py 11 12 13 def fit ( self , X , y ) -> \"Identity\" : \"\"\"Fits the estimator. No-op.\"\"\" return self Fits the estimator. No-op. fit_transform ( self , X , y = None , ** fit_params ) \u00b6 Show source code in preprocessing/_identity.py 19 20 21 def fit_transform ( self , X , y = None , ** fit_params ) -> Any : \"\"\"Transforms the data per scikit-learn API.\"\"\" return self . fit ( X , y ) . transform ( X ) Transforms the data per scikit-learn API. transform ( self , X ) \u00b6 Show source code in preprocessing/_identity.py 15 16 17 def transform ( self , X : Any ) -> Any : \"\"\"Transforms the data per scikit-learn API.\"\"\" return X Transforms the data per scikit-learn API. ColumnLister \u00b6 Takes a pandas column as a list of text. fit ( self , X , y ) \u00b6 Show source code in preprocessing/_columnlist.py 11 12 13 def fit ( self , X , y ) -> \"ColumnLister\" : \"\"\"Fits the estimator. No-op.\"\"\" return self Fits the estimator. No-op. fit_transform ( self , X , y = None , ** fit_params ) \u00b6 Show source code in preprocessing/_columnlist.py 19 20 21 def fit_transform ( self , X , y = None , ** fit_params ) -> List [ str ]: \"\"\"Transforms the data per scikit-learn API.\"\"\" return self . fit ( X , y ) . transform ( X ) Transforms the data per scikit-learn API. transform ( self , X ) \u00b6 Show source code in preprocessing/_columnlist.py 15 16 17 def transform ( self , X ) -> List [ str ]: \"\"\"Transforms the data per scikit-learn API.\"\"\" return X [ self . column ] . to_list () Transforms the data per scikit-learn API.","title":"simsity.preprocessing"},{"location":"api/preprocessing/#identity","text":"Encoder/Transformer that keeps data as-is.","title":"Identity"},{"location":"api/preprocessing/#simsity.preprocessing._identity.Identity.fit","text":"Show source code in preprocessing/_identity.py 11 12 13 def fit ( self , X , y ) -> \"Identity\" : \"\"\"Fits the estimator. No-op.\"\"\" return self Fits the estimator. No-op.","title":"fit()"},{"location":"api/preprocessing/#simsity.preprocessing._identity.Identity.fit_transform","text":"Show source code in preprocessing/_identity.py 19 20 21 def fit_transform ( self , X , y = None , ** fit_params ) -> Any : \"\"\"Transforms the data per scikit-learn API.\"\"\" return self . fit ( X , y ) . transform ( X ) Transforms the data per scikit-learn API.","title":"fit_transform()"},{"location":"api/preprocessing/#simsity.preprocessing._identity.Identity.transform","text":"Show source code in preprocessing/_identity.py 15 16 17 def transform ( self , X : Any ) -> Any : \"\"\"Transforms the data per scikit-learn API.\"\"\" return X Transforms the data per scikit-learn API.","title":"transform()"},{"location":"api/preprocessing/#columnlister","text":"Takes a pandas column as a list of text.","title":"ColumnLister"},{"location":"api/preprocessing/#simsity.preprocessing._columnlist.ColumnLister.fit","text":"Show source code in preprocessing/_columnlist.py 11 12 13 def fit ( self , X , y ) -> \"ColumnLister\" : \"\"\"Fits the estimator. No-op.\"\"\" return self Fits the estimator. No-op.","title":"fit()"},{"location":"api/preprocessing/#simsity.preprocessing._columnlist.ColumnLister.fit_transform","text":"Show source code in preprocessing/_columnlist.py 19 20 21 def fit_transform ( self , X , y = None , ** fit_params ) -> List [ str ]: \"\"\"Transforms the data per scikit-learn API.\"\"\" return self . fit ( X , y ) . transform ( X ) Transforms the data per scikit-learn API.","title":"fit_transform()"},{"location":"api/preprocessing/#simsity.preprocessing._columnlist.ColumnLister.transform","text":"Show source code in preprocessing/_columnlist.py 15 16 17 def transform ( self , X ) -> List [ str ]: \"\"\"Transforms the data per scikit-learn API.\"\"\" return X [ self . column ] . to_list () Transforms the data per scikit-learn API.","title":"transform()"},{"location":"api/service/","text":"from simsity.service import Service \u00b6 This object represents a nearest neighbor lookup service. You can pass it an encoder and a method to index the data. Parameters Name Type Description Default encoder A scikit-learn compatible encoder for the input. Identity() indexer Indexer A compatible indexer for the nearest neighbor search. None storage A dictionary containing the data to be retreived with index. Meant to be ignored by humans. None load ( path ) (classmethod) \u00b6 Show source code in simsity/service.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 @classmethod def load ( cls , path ): \"\"\" Loads a service Arguments: path: Path to the folder to load the service from. \"\"\" if not pathlib . Path ( path ) . exists (): raise FileNotFoundError ( f \" { path } does not exist\" ) metadata_path = pathlib . Path ( path ) / \"metadata.json\" metadata = json . loads ( metadata_path . read_text ()) if metadata [ \"version\" ] != __version__ : raise RuntimeError ( f \"Version mismatch. Expected { __version__ } , got { metadata [ 'version' ] } \" ) storage_path = pathlib . Path ( path ) / \"storage.json\" storage = { int ( k ): v for k , v in json . loads ( storage_path . read_text ()) . items ()} encoder = load ( pathlib . Path ( path ) / \"encoder.joblib\" ) indexer = cls . _load_indexer ( pathlib . Path ( path ), metadata ) service = cls ( encoder , indexer , storage ) service . _trained = True return service Loads a service Parameters Name Type Description Default path Path to the folder to load the service from. required query ( self , n_neighbors = 10 , out = 'list' , ** kwargs ) \u00b6 Show source code in simsity/service.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def query ( self , n_neighbors = 10 , out = \"list\" , ** kwargs ): \"\"\" Query the service. Arguments: n_neighbors: Number of neighbors to return. out: Output format. Can be either \"list\" or \"dataframe\". kwargs: Arguments to pass as the query. \"\"\" if not self . _trained : raise RuntimeError ( \"Cannot query, Service is not trained.\" ) if n_neighbors > len ( self . storage ): raise ValueError ( \"n_neighbors cannot be greater than the number of items in the storage.\" ) data = self . encoder . transform ( pd . DataFrame ([{ ** kwargs }])) idx , dist = self . indexer . query ( data , n_neighbors = n_neighbors ) res = [ { \"item\" : self . storage [ idx [ i ]], \"dist\" : float ( dist [ i ])} for i in range ( len ( idx )) ] if out == \"list\" : return res if out == \"dataframe\" : return pd . DataFrame ([{ ** r [ \"item\" ], \"dist\" : r [ \"dist\" ]} for r in res ]) Query the service. Parameters Name Type Description Default n_neighbors Number of neighbors to return. 10 out Output format. Can be either \"list\" or \"dataframe\". 'list' **kwargs Arguments to pass as the query. {} save ( self , path ) \u00b6 Show source code in simsity/service.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def save ( self , path ): \"\"\" Save the service Arguments: path: Path to the folder to save the service to. \"\"\" if not self . _trained : raise RuntimeError ( \"Cannot save, Service is not trained.\" ) pathlib . Path ( path ) . mkdir ( parents = True , exist_ok = True ) storage_path = pathlib . Path ( path ) / \"storage.json\" storage_path . write_text ( json . dumps ( self . storage )) metadata_path = pathlib . Path ( path ) / \"metadata.json\" metadata_path . write_text ( json . dumps ({ \"version\" : __version__ })) dump ( self . encoder , pathlib . Path ( path ) / \"encoder.joblib\" ) self . indexer . save ( pathlib . Path ( path )) Save the service Parameters Name Type Description Default path Path to the folder to save the service to. required serve ( self , host , port = 8080 ) \u00b6 Show source code in simsity/service.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 def serve ( self , host , port = 8080 ): \"\"\" Start a server for the service. Once the server is started, you can `POST` the service using the following URL: ``` http://<host>:<port>/query ``` Arguments: host: Host to bind the server to. port: Port to bind the server to. \"\"\" import uvicorn from simsity.serve import create_app uvicorn . run ( create_app ( self ), host = host , port = port ) Start a server for the service. Once the server is started, you can POST the service using the following URL: http://<host>:<port>/query Parameters Name Type Description Default host Host to bind the server to. required port Port to bind the server to. 8080 train_from_dataf ( self , df , features = None ) \u00b6 Show source code in simsity/service.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def train_from_dataf ( self , df , features = None ): \"\"\" Trains the service from a dataframe. Arguments: df: Pandas DataFrame that contains text to train the service with. features: Names of the features to encode. \"\"\" subset = df if features : subset = df [ features ] self . storage = { i : r for i , r in enumerate ( subset . to_dict ( orient = \"records\" ))} if not self . _trained : self . encoder . fit ( subset , y = None ) try : data = self . encoder . transform ( subset ) except Exception as e : warnings . warn ( \"Encountered error using pretrained encoder. Are you sure it is trained?\" ) raise e self . indexer . index ( data ) self . _trained = True return self Trains the service from a dataframe. Parameters Name Type Description Default df Pandas DataFrame that contains text to train the service with. required features Names of the features to encode. None","title":"simsity.service"},{"location":"api/service/#from-simsityservice-import-service","text":"This object represents a nearest neighbor lookup service. You can pass it an encoder and a method to index the data. Parameters Name Type Description Default encoder A scikit-learn compatible encoder for the input. Identity() indexer Indexer A compatible indexer for the nearest neighbor search. None storage A dictionary containing the data to be retreived with index. Meant to be ignored by humans. None","title":"from simsity.service import Service"},{"location":"api/service/#simsity.service.Service.load","text":"Show source code in simsity/service.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 @classmethod def load ( cls , path ): \"\"\" Loads a service Arguments: path: Path to the folder to load the service from. \"\"\" if not pathlib . Path ( path ) . exists (): raise FileNotFoundError ( f \" { path } does not exist\" ) metadata_path = pathlib . Path ( path ) / \"metadata.json\" metadata = json . loads ( metadata_path . read_text ()) if metadata [ \"version\" ] != __version__ : raise RuntimeError ( f \"Version mismatch. Expected { __version__ } , got { metadata [ 'version' ] } \" ) storage_path = pathlib . Path ( path ) / \"storage.json\" storage = { int ( k ): v for k , v in json . loads ( storage_path . read_text ()) . items ()} encoder = load ( pathlib . Path ( path ) / \"encoder.joblib\" ) indexer = cls . _load_indexer ( pathlib . Path ( path ), metadata ) service = cls ( encoder , indexer , storage ) service . _trained = True return service Loads a service Parameters Name Type Description Default path Path to the folder to load the service from. required","title":"load()"},{"location":"api/service/#simsity.service.Service.query","text":"Show source code in simsity/service.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def query ( self , n_neighbors = 10 , out = \"list\" , ** kwargs ): \"\"\" Query the service. Arguments: n_neighbors: Number of neighbors to return. out: Output format. Can be either \"list\" or \"dataframe\". kwargs: Arguments to pass as the query. \"\"\" if not self . _trained : raise RuntimeError ( \"Cannot query, Service is not trained.\" ) if n_neighbors > len ( self . storage ): raise ValueError ( \"n_neighbors cannot be greater than the number of items in the storage.\" ) data = self . encoder . transform ( pd . DataFrame ([{ ** kwargs }])) idx , dist = self . indexer . query ( data , n_neighbors = n_neighbors ) res = [ { \"item\" : self . storage [ idx [ i ]], \"dist\" : float ( dist [ i ])} for i in range ( len ( idx )) ] if out == \"list\" : return res if out == \"dataframe\" : return pd . DataFrame ([{ ** r [ \"item\" ], \"dist\" : r [ \"dist\" ]} for r in res ]) Query the service. Parameters Name Type Description Default n_neighbors Number of neighbors to return. 10 out Output format. Can be either \"list\" or \"dataframe\". 'list' **kwargs Arguments to pass as the query. {}","title":"query()"},{"location":"api/service/#simsity.service.Service.save","text":"Show source code in simsity/service.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def save ( self , path ): \"\"\" Save the service Arguments: path: Path to the folder to save the service to. \"\"\" if not self . _trained : raise RuntimeError ( \"Cannot save, Service is not trained.\" ) pathlib . Path ( path ) . mkdir ( parents = True , exist_ok = True ) storage_path = pathlib . Path ( path ) / \"storage.json\" storage_path . write_text ( json . dumps ( self . storage )) metadata_path = pathlib . Path ( path ) / \"metadata.json\" metadata_path . write_text ( json . dumps ({ \"version\" : __version__ })) dump ( self . encoder , pathlib . Path ( path ) / \"encoder.joblib\" ) self . indexer . save ( pathlib . Path ( path )) Save the service Parameters Name Type Description Default path Path to the folder to save the service to. required","title":"save()"},{"location":"api/service/#simsity.service.Service.serve","text":"Show source code in simsity/service.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 def serve ( self , host , port = 8080 ): \"\"\" Start a server for the service. Once the server is started, you can `POST` the service using the following URL: ``` http://<host>:<port>/query ``` Arguments: host: Host to bind the server to. port: Port to bind the server to. \"\"\" import uvicorn from simsity.serve import create_app uvicorn . run ( create_app ( self ), host = host , port = port ) Start a server for the service. Once the server is started, you can POST the service using the following URL: http://<host>:<port>/query Parameters Name Type Description Default host Host to bind the server to. required port Port to bind the server to. 8080","title":"serve()"},{"location":"api/service/#simsity.service.Service.train_from_dataf","text":"Show source code in simsity/service.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def train_from_dataf ( self , df , features = None ): \"\"\" Trains the service from a dataframe. Arguments: df: Pandas DataFrame that contains text to train the service with. features: Names of the features to encode. \"\"\" subset = df if features : subset = df [ features ] self . storage = { i : r for i , r in enumerate ( subset . to_dict ( orient = \"records\" ))} if not self . _trained : self . encoder . fit ( subset , y = None ) try : data = self . encoder . transform ( subset ) except Exception as e : warnings . warn ( \"Encountered error using pretrained encoder. Are you sure it is trained?\" ) raise e self . indexer . index ( data ) self . _trained = True return self Trains the service from a dataframe. Parameters Name Type Description Default df Pandas DataFrame that contains text to train the service with. required features Names of the features to encode. None","title":"train_from_dataf()"},{"location":"quickstart/","text":"The goal of this tool is to offer you a simple service for similarity detection. So let's build an example! We'll build a similarity searching tool for a text dataset. Example Data \u00b6 Let's look at the clinc dataset. This dataset contains texts that represent intents that you might expect from a chatbot. from simsity.datasets import fetch_clinc df = fetch_clinc () Here's what the top 5 rows look like. text label split get a text to mark 26 valid how do i let my bank know i'm going to vietnam 146 train what location did you live at before 77 train i got to remove fishing from my calendar for soccer 105 train i need an uber to class tonight on the mainline campus 39 train We see a bunch of information in this dataset but we're interested in exploring similarities in the text field. The Tactic \u00b6 The idea is that we're going to split the problem of similarity search into two subproblems. The first problem is encoding . If we're going to use similarities, we'll need some way to turn our data into a numeric representation. Without a numeric representation it'll be quite hard to compare items numerically. The second problem is indexing . Even when we have numeric representations to compare against, we don't want to compare all the possible solutions out there. Instead we'd prefer to index out data such that it's fast to retreive. To solve the first problem, simsity likes to re-use tools from the scikit-learn ecosystem. An encoder in simsity is simply a scikit-learn pipeline that transforms data. To solve the second problem, simsity wraps around existing tools for approximate nearest-neighbor lookup. The goal of simsity is to combine an encoder and an indexer into a service that's convenient for interaction. Example Encoder \u00b6 We're going to encode text, so a straightforward encoder would be the countvectorizer from scikit-learn . This way, we will index each word in the text in a sparse array. This sparse array will then get indexed by our indexer later. Here's the code required to run this. from simsity.preprocessing import ColumnLister from sklearn.pipeline import make_pipeline from sklearn.feature_extraction.text import CountVectorizer encoder = make_pipeline ( ColumnLister ( column = \"text\" ), CountVectorizer ()) You may wonder why we're building a pipeline with a ColumnLister class. The simple story is that the CountVectorizer needs a list of strings as input. The only thing that the ColumnLister does is that it takes the dataframe and extracts the \"text\" column and outputs it as a list. By wrapping both components in a make_pipeline -call we're still dealing with a single encoder pipeline that we can pass along. Example Indexer \u00b6 Simsity provides indexers by wrapping around existing solutions. In particular it supports PyNNDescent out of the box. from simsity.indexer import PyNNDescentIndexer indexer = PyNNDescentIndexer ( metric = \"euclidean\" , n_jobs = 6 ) There are many distance metrics that PyNNDescent supports and it's also able to index in parallel by setting the n_jobs parameter. Note We'll be using the PyNNDescentIndexer indexer in this demo because it's very flexible. It supports dense arrays as well as sparse ones! The only downside is that it does take a while to index all the data. If you're looking for a faster indexing method you may want to try the AnnoyIndexer based on annoy . If you're curious to learn how it works, you may appreciate this segment on calmcode . Building a Service \u00b6 Once you have an encoder and an indexer, you can construct a service. from simsity.service import Service service = Service ( indexer = indexer , encoder = encoder ) This service can now train on your dataset. It will start by first training the encoder pipeline. After that the data will be transformed and indexed by the indexer. All of this will be handled by the following call: service . train_from_dataf ( df , features = [ \"text\" ]) It's good to notice that we're being explicit here about which features are being used. We're telling our service to only consider the \"text\" column! This is important when you want to query your data. Query the Data \u00b6 You can now try to look for similar items by sending a query to the service. Note that the keyword argument text= corresponds with the features that we chose to index earlier. service . query ( text = \"please set a timer\" , n_neighbors = 10 , out = \"dataframe\" ) This is the table that you'll get back. text dist please set a 4 minute timer 1 set a timer 1 please set a timer for 1 \"please set 5 minute timer 1 start a timer please 1.41421 please begin a timer 1.41421 set a 2 minute timer 1.41421 set a 4 minute timer 1.41421 set a 1 minute timer 1.41421 set a five minute timer please 1.41421 The quality of what you get back depends on the data that you give the system, the encoding and the indexer that you pick. It may very well be that we should add word embeddings via whatlies or that we should consider a different metric when we index our data. It's also very well possible that text that is too long or too short won't compare easily. The goal of this package is to make it easy to interact and experiment with the idea of \"building neigborhoods of similar items\". Hence the name: simsity. Extra Features \u00b6 Training a service may take quite a bit of time, so it may be good to save your service on disk. service . save ( \"/tmp/simple-model\" ) You can reload the service by using the .load classmethod. reloaded = Service . load ( \"/tmp/simple-model\" ) You could even run it as a webservice if you were so inclined. reloaded . serve ( host = '0.0.0.0' , port = 8080 ) You can now POST to http://0.0.0.0:8080/query with payload: {\"query\": {\"text\": \"please set a timer\"}, \"n_neighbors\": 5} This would be the response that you get back. [{'item': {'text': 'set a timer'}, 'dist': 1.0}, {'item': {'text': 'please set a timer for'}, 'dist': 1.0}, {'item': {'text': '\"please set 5 minute timer'}, 'dist': 1.0}, {'item': {'text': 'please set a 4 minute timer'}, 'dist': 1.0}, {'item': {'text': 'start a timer please'}, 'dist': 1.4142135381698608}] All Code \u00b6 Here's the full code block that we've used in this section. from simsity.service import Service from simsity.datasets import fetch_clinc from simsity.indexer import PyNNDescentIndexer from simsity.preprocessing import ColumnLister from sklearn.pipeline import make_pipeline from sklearn.feature_extraction.text import CountVectorizer # We first fetch a dataset df = fetch_clinc () # The Encoder handles the encoding of the datapoints encoder = make_pipeline ( ColumnLister ( column = \"text\" ), CountVectorizer ()) # The Indexer handles the nearest neighbor search indexer = PyNNDescentIndexer ( metric = \"euclidean\" , n_jobs = 6 ) # The Service combines them together into a service service = Service ( indexer = indexer , encoder = encoder ) # Important for later: we're only passing the 'text' column to encode service . train_from_dataf ( df , features = [ \"text\" ]) # Query the datapoints # Note that the keyword argument here refers to 'text'-column service . query ( text = \"please set a timer\" , n_neighbors = 10 , out = \"dataframe\" ) # Save the entire system service . save ( \"/tmp/simple-model\" ) # You can also load the model now. reloaded = Service . load ( \"/tmp/simple-model\" ) reloaded . serve ( host = '0.0.0.0' , port = 8080 )","title":"Quickstart"},{"location":"quickstart/#example-data","text":"Let's look at the clinc dataset. This dataset contains texts that represent intents that you might expect from a chatbot. from simsity.datasets import fetch_clinc df = fetch_clinc () Here's what the top 5 rows look like. text label split get a text to mark 26 valid how do i let my bank know i'm going to vietnam 146 train what location did you live at before 77 train i got to remove fishing from my calendar for soccer 105 train i need an uber to class tonight on the mainline campus 39 train We see a bunch of information in this dataset but we're interested in exploring similarities in the text field.","title":"Example Data"},{"location":"quickstart/#the-tactic","text":"The idea is that we're going to split the problem of similarity search into two subproblems. The first problem is encoding . If we're going to use similarities, we'll need some way to turn our data into a numeric representation. Without a numeric representation it'll be quite hard to compare items numerically. The second problem is indexing . Even when we have numeric representations to compare against, we don't want to compare all the possible solutions out there. Instead we'd prefer to index out data such that it's fast to retreive. To solve the first problem, simsity likes to re-use tools from the scikit-learn ecosystem. An encoder in simsity is simply a scikit-learn pipeline that transforms data. To solve the second problem, simsity wraps around existing tools for approximate nearest-neighbor lookup. The goal of simsity is to combine an encoder and an indexer into a service that's convenient for interaction.","title":"The Tactic"},{"location":"quickstart/#example-encoder","text":"We're going to encode text, so a straightforward encoder would be the countvectorizer from scikit-learn . This way, we will index each word in the text in a sparse array. This sparse array will then get indexed by our indexer later. Here's the code required to run this. from simsity.preprocessing import ColumnLister from sklearn.pipeline import make_pipeline from sklearn.feature_extraction.text import CountVectorizer encoder = make_pipeline ( ColumnLister ( column = \"text\" ), CountVectorizer ()) You may wonder why we're building a pipeline with a ColumnLister class. The simple story is that the CountVectorizer needs a list of strings as input. The only thing that the ColumnLister does is that it takes the dataframe and extracts the \"text\" column and outputs it as a list. By wrapping both components in a make_pipeline -call we're still dealing with a single encoder pipeline that we can pass along.","title":"Example Encoder"},{"location":"quickstart/#example-indexer","text":"Simsity provides indexers by wrapping around existing solutions. In particular it supports PyNNDescent out of the box. from simsity.indexer import PyNNDescentIndexer indexer = PyNNDescentIndexer ( metric = \"euclidean\" , n_jobs = 6 ) There are many distance metrics that PyNNDescent supports and it's also able to index in parallel by setting the n_jobs parameter. Note We'll be using the PyNNDescentIndexer indexer in this demo because it's very flexible. It supports dense arrays as well as sparse ones! The only downside is that it does take a while to index all the data. If you're looking for a faster indexing method you may want to try the AnnoyIndexer based on annoy . If you're curious to learn how it works, you may appreciate this segment on calmcode .","title":"Example Indexer"},{"location":"quickstart/#building-a-service","text":"Once you have an encoder and an indexer, you can construct a service. from simsity.service import Service service = Service ( indexer = indexer , encoder = encoder ) This service can now train on your dataset. It will start by first training the encoder pipeline. After that the data will be transformed and indexed by the indexer. All of this will be handled by the following call: service . train_from_dataf ( df , features = [ \"text\" ]) It's good to notice that we're being explicit here about which features are being used. We're telling our service to only consider the \"text\" column! This is important when you want to query your data.","title":"Building a Service"},{"location":"quickstart/#query-the-data","text":"You can now try to look for similar items by sending a query to the service. Note that the keyword argument text= corresponds with the features that we chose to index earlier. service . query ( text = \"please set a timer\" , n_neighbors = 10 , out = \"dataframe\" ) This is the table that you'll get back. text dist please set a 4 minute timer 1 set a timer 1 please set a timer for 1 \"please set 5 minute timer 1 start a timer please 1.41421 please begin a timer 1.41421 set a 2 minute timer 1.41421 set a 4 minute timer 1.41421 set a 1 minute timer 1.41421 set a five minute timer please 1.41421 The quality of what you get back depends on the data that you give the system, the encoding and the indexer that you pick. It may very well be that we should add word embeddings via whatlies or that we should consider a different metric when we index our data. It's also very well possible that text that is too long or too short won't compare easily. The goal of this package is to make it easy to interact and experiment with the idea of \"building neigborhoods of similar items\". Hence the name: simsity.","title":"Query the Data"},{"location":"quickstart/#extra-features","text":"Training a service may take quite a bit of time, so it may be good to save your service on disk. service . save ( \"/tmp/simple-model\" ) You can reload the service by using the .load classmethod. reloaded = Service . load ( \"/tmp/simple-model\" ) You could even run it as a webservice if you were so inclined. reloaded . serve ( host = '0.0.0.0' , port = 8080 ) You can now POST to http://0.0.0.0:8080/query with payload: {\"query\": {\"text\": \"please set a timer\"}, \"n_neighbors\": 5} This would be the response that you get back. [{'item': {'text': 'set a timer'}, 'dist': 1.0}, {'item': {'text': 'please set a timer for'}, 'dist': 1.0}, {'item': {'text': '\"please set 5 minute timer'}, 'dist': 1.0}, {'item': {'text': 'please set a 4 minute timer'}, 'dist': 1.0}, {'item': {'text': 'start a timer please'}, 'dist': 1.4142135381698608}]","title":"Extra Features"},{"location":"quickstart/#all-code","text":"Here's the full code block that we've used in this section. from simsity.service import Service from simsity.datasets import fetch_clinc from simsity.indexer import PyNNDescentIndexer from simsity.preprocessing import ColumnLister from sklearn.pipeline import make_pipeline from sklearn.feature_extraction.text import CountVectorizer # We first fetch a dataset df = fetch_clinc () # The Encoder handles the encoding of the datapoints encoder = make_pipeline ( ColumnLister ( column = \"text\" ), CountVectorizer ()) # The Indexer handles the nearest neighbor search indexer = PyNNDescentIndexer ( metric = \"euclidean\" , n_jobs = 6 ) # The Service combines them together into a service service = Service ( indexer = indexer , encoder = encoder ) # Important for later: we're only passing the 'text' column to encode service . train_from_dataf ( df , features = [ \"text\" ]) # Query the datapoints # Note that the keyword argument here refers to 'text'-column service . query ( text = \"please set a timer\" , n_neighbors = 10 , out = \"dataframe\" ) # Save the entire system service . save ( \"/tmp/simple-model\" ) # You can also load the model now. reloaded = Service . load ( \"/tmp/simple-model\" ) reloaded . serve ( host = '0.0.0.0' , port = 8080 )","title":"All Code"},{"location":"quickstart/cool/","text":"This document explores some cool/useful tricks you can pull off with this library. Benchmarking \u00b6 Before diving into benchmarking, we should be acknowledge that coming up with meaningful benchmarks is hard. The goal of this document is to inspire folks to think about benchmarking, not to suggest that this page highlights a state of the art result. Having said that ... let's say that you're interested in building a retreival service and you happen to have a dataset that's labelled. In that case you may be able to calculate precision-at-k and recall-at-k ( wiki ). The examples/benchmark.ipynb file on the GitHub repository shows a full example using the clinc-dataset. The benchmark compares two encoders. One is fairly basic and only tracks word-tokens while the other includes subword embeddings and countvectors. # Original Encoder Pipeline encoder = make_pipeline ( ColumnLister ( 'text' ), CountVectorizer () ) # New Encoder Pipeline encoder = make_pipeline ( ColumnLister ( 'text' ), make_union ( CountVectorizer (), CountVectorizer ( analyzer = \"char\" , ngram_range = ( 2 , 3 )), BytePairLanguage ( \"en\" , vs = 1_000 ), BytePairLanguage ( \"en\" , vs = 100_000 ), ) ) The results from the comparison are summarised in the chart below. Feel free to click/drag/hover/zoom. You can double-click to reset the view. Again, we don't want to suggest that the encoders that we used are state of the art, but we do hope the notebook offers a convenient starting point for folks to start benchmarking experiments. Interactive Widgets \u00b6 For extra interactivity you may be interested in using simsity with interactive jupyter widgets. To use these, you'll want to double check that you're using a modern jupyterlab installation and that the ipywidgets library is installed. pip install ipywidgets jupyter nbextension enable --py widgetsnbextension You can now re-use a service as an interactive widget. import ipywidgets as widgets def reduce ( q ): subset = service . query ( text = q , n_neighbors = 15 , out = \"dataframe\" ) display ( subset ) q = widgets . Text () out = widgets . interactive_output ( reduce , { 'q' : q }) widgets . VBox ([ q , out ]) Here's what the experience is like: If you're unfamiliar with the widgets and appreciate a course we recommend checking this calmcode.io course . Labelling Aid \u00b6 Suppose you've started working on a deduplication use-case. Then odds are that you don't have a ground truth of labels just yet. So how might simsity help out in such a use-case? If you're looking for a simple way to label inside of a jupyter notebook you can use pigeon . There's a course here if you're unfamiliar. Make sure it's installed beforehand via: pip install pigeon-jupyter Let's demonstrate how you can use a service here. import pandas as pd from simsity.service import Service from simsity.datasets import fetch_voters from simsity.indexer import PyNNDescentIndexer # Don't forget to pip install dirty_cat from dirty_cat import GapEncoder df = fetch_voters () encoder = GapEncoder () service = Service ( indexer = PyNNDescentIndexer ( metric = \"euclidean\" ), encoder = encoder ) service . train_from_dataf ( df ) Given that we have a service that can find similar items. We now need a function that can generate candidates that may be similar. import random def generate_pair ( service , n_consider = 10 ): idx = random . randint ( 0 , len ( service . storage ) - 1 ) query = service . storage [ idx ] df_out = service . query ( ** query , n_neighbors = n_consider , out = 'dataframe' ) return df_out . drop ( columns = 'dist' ) . sample ( 2 ) generate_pair ( service ) This function will generate a dataframe that contains a pair of potentially similar items. This function can be used in a pigeon loop for labelling. from IPython.display import display from pigeon import annotate annotations = annotate ( ( generate_pair ( service , n_consider = 3 ) for x in range ( 200 )), options = [ 'similar' , 'not similar' ], display_fn = display ) Here's what the experience is like. Machine Learning \u00b6 After generating annotations you can inspect them. Here's a helper function that can give an overview of the labels. def annot_to_dataf ( annotations ): data = [] for a in annotations : d1 , d2 = a [ 0 ] . to_dict ( orient = 'records' ) d1 = { f ' { k } _1' : v for k , v in d1 . items ()} d2 = { f ' { k } _2' : v for k , v in d2 . items ()} data . append ({ ** d1 , ** d2 , 'label' : a [ 1 ]}) return pd . DataFrame ( data ) annot_to_dataf ( annotations ) Here's an example of what you might see. name_1 suburb_1 postcode_1 name_2 suburb_2 postcode_2 label jamara bigger gastonia 28054 jeffrey cox flexer 28702 not similar ashlee kreer hickry 28601 frank lea new bern 28562 not similar perry kirksey leland 28451 james skipper rocky point 28457 not similar jimmy penny pittsboro 27312 audrey noel greensboro 27405 not similar terri foster yanceyville 27379 ter5i foster yankeyville 27379 similar Given these labels, you can now try to train a model that can detect if two rows are \"similar\". We can re-use the encoders that we used before. def annot_to_X_y ( annotations , encoder ): data1 = [] data2 = [] ys = [] for a in annotations : d1 , d2 = a [ 0 ] . to_dict ( orient = 'records' ) data1 . append ( d1 ) data2 . append ( d2 ) ys . append ( a [ 1 ]) X1 = encoder . transform ( pd . DataFrame ( data1 )) X2 = encoder . transform ( pd . DataFrame ( data2 )) return X1 , X2 , ys X1 , X2 , y = annot_to_X_y ( annotations , encoder ) The X1 and X2 arrays resemble the encodings of the rows that we labelled before. We could take the difference between these two rows and leave it to a machine learning algorithm to figure out which items in the array matter when it comes to \"being similar\". import numpy as np from sklearn.linear_model import LogisticRegression X_difference = X1 - X2 model = LogisticRegression () . fit ( X_difference , y ) pred = model . predict ( X_difference ) # This is the train performance np . mean ( pred == y ) The more data you collect, the better this model may become. Once you're satisfied with the performance you can use the original service to find items that are \"close\" and you can use this model to pluck out the candidates that matter. The goal is to save the annotator some time while doing this.","title":"Cool Tricks"},{"location":"quickstart/cool/#benchmarking","text":"Before diving into benchmarking, we should be acknowledge that coming up with meaningful benchmarks is hard. The goal of this document is to inspire folks to think about benchmarking, not to suggest that this page highlights a state of the art result. Having said that ... let's say that you're interested in building a retreival service and you happen to have a dataset that's labelled. In that case you may be able to calculate precision-at-k and recall-at-k ( wiki ). The examples/benchmark.ipynb file on the GitHub repository shows a full example using the clinc-dataset. The benchmark compares two encoders. One is fairly basic and only tracks word-tokens while the other includes subword embeddings and countvectors. # Original Encoder Pipeline encoder = make_pipeline ( ColumnLister ( 'text' ), CountVectorizer () ) # New Encoder Pipeline encoder = make_pipeline ( ColumnLister ( 'text' ), make_union ( CountVectorizer (), CountVectorizer ( analyzer = \"char\" , ngram_range = ( 2 , 3 )), BytePairLanguage ( \"en\" , vs = 1_000 ), BytePairLanguage ( \"en\" , vs = 100_000 ), ) ) The results from the comparison are summarised in the chart below. Feel free to click/drag/hover/zoom. You can double-click to reset the view. Again, we don't want to suggest that the encoders that we used are state of the art, but we do hope the notebook offers a convenient starting point for folks to start benchmarking experiments.","title":"Benchmarking"},{"location":"quickstart/cool/#interactive-widgets","text":"For extra interactivity you may be interested in using simsity with interactive jupyter widgets. To use these, you'll want to double check that you're using a modern jupyterlab installation and that the ipywidgets library is installed. pip install ipywidgets jupyter nbextension enable --py widgetsnbextension You can now re-use a service as an interactive widget. import ipywidgets as widgets def reduce ( q ): subset = service . query ( text = q , n_neighbors = 15 , out = \"dataframe\" ) display ( subset ) q = widgets . Text () out = widgets . interactive_output ( reduce , { 'q' : q }) widgets . VBox ([ q , out ]) Here's what the experience is like: If you're unfamiliar with the widgets and appreciate a course we recommend checking this calmcode.io course .","title":"Interactive Widgets"},{"location":"quickstart/cool/#labelling-aid","text":"Suppose you've started working on a deduplication use-case. Then odds are that you don't have a ground truth of labels just yet. So how might simsity help out in such a use-case? If you're looking for a simple way to label inside of a jupyter notebook you can use pigeon . There's a course here if you're unfamiliar. Make sure it's installed beforehand via: pip install pigeon-jupyter Let's demonstrate how you can use a service here. import pandas as pd from simsity.service import Service from simsity.datasets import fetch_voters from simsity.indexer import PyNNDescentIndexer # Don't forget to pip install dirty_cat from dirty_cat import GapEncoder df = fetch_voters () encoder = GapEncoder () service = Service ( indexer = PyNNDescentIndexer ( metric = \"euclidean\" ), encoder = encoder ) service . train_from_dataf ( df ) Given that we have a service that can find similar items. We now need a function that can generate candidates that may be similar. import random def generate_pair ( service , n_consider = 10 ): idx = random . randint ( 0 , len ( service . storage ) - 1 ) query = service . storage [ idx ] df_out = service . query ( ** query , n_neighbors = n_consider , out = 'dataframe' ) return df_out . drop ( columns = 'dist' ) . sample ( 2 ) generate_pair ( service ) This function will generate a dataframe that contains a pair of potentially similar items. This function can be used in a pigeon loop for labelling. from IPython.display import display from pigeon import annotate annotations = annotate ( ( generate_pair ( service , n_consider = 3 ) for x in range ( 200 )), options = [ 'similar' , 'not similar' ], display_fn = display ) Here's what the experience is like.","title":"Labelling Aid"},{"location":"quickstart/cool/#machine-learning","text":"After generating annotations you can inspect them. Here's a helper function that can give an overview of the labels. def annot_to_dataf ( annotations ): data = [] for a in annotations : d1 , d2 = a [ 0 ] . to_dict ( orient = 'records' ) d1 = { f ' { k } _1' : v for k , v in d1 . items ()} d2 = { f ' { k } _2' : v for k , v in d2 . items ()} data . append ({ ** d1 , ** d2 , 'label' : a [ 1 ]}) return pd . DataFrame ( data ) annot_to_dataf ( annotations ) Here's an example of what you might see. name_1 suburb_1 postcode_1 name_2 suburb_2 postcode_2 label jamara bigger gastonia 28054 jeffrey cox flexer 28702 not similar ashlee kreer hickry 28601 frank lea new bern 28562 not similar perry kirksey leland 28451 james skipper rocky point 28457 not similar jimmy penny pittsboro 27312 audrey noel greensboro 27405 not similar terri foster yanceyville 27379 ter5i foster yankeyville 27379 similar Given these labels, you can now try to train a model that can detect if two rows are \"similar\". We can re-use the encoders that we used before. def annot_to_X_y ( annotations , encoder ): data1 = [] data2 = [] ys = [] for a in annotations : d1 , d2 = a [ 0 ] . to_dict ( orient = 'records' ) data1 . append ( d1 ) data2 . append ( d2 ) ys . append ( a [ 1 ]) X1 = encoder . transform ( pd . DataFrame ( data1 )) X2 = encoder . transform ( pd . DataFrame ( data2 )) return X1 , X2 , ys X1 , X2 , y = annot_to_X_y ( annotations , encoder ) The X1 and X2 arrays resemble the encodings of the rows that we labelled before. We could take the difference between these two rows and leave it to a machine learning algorithm to figure out which items in the array matter when it comes to \"being similar\". import numpy as np from sklearn.linear_model import LogisticRegression X_difference = X1 - X2 model = LogisticRegression () . fit ( X_difference , y ) pred = model . predict ( X_difference ) # This is the train performance np . mean ( pred == y ) The more data you collect, the better this model may become. Once you're satisfied with the performance you can use the original service to find items that are \"close\" and you can use this model to pluck out the candidates that matter. The goal is to save the annotator some time while doing this.","title":"Machine Learning"},{"location":"quickstart/dedup/","text":"An interesting use-case for simsity is to use it as a tool that explores deduplication. Example \u00b6 Let's consider the voters dataset. from simsity.datasets import fetch_voters df = fetch_voters () name suburb postcode khimerc thomas charlotte 2826g lucille richardst kannapolis 28o81 reb3cca bauerboand raleigh 27615 maleda mccloud goldsboro 2753o belida stovall morrisvill 27560 This dataset contains information about \"voters\" and the concern is that some of these rows may represent the same person. The persons name might occur in different spellings and the postcodes may contain typos, but they could still refer to the same person. In other words; there may be duplicates in this dataframe that we cannot remove with .drop_duplicates() . So how might we go about finding these? Similarity Service! \u00b6 Let's build a similarity service, but now we'll use encoders from the dirty_cat package. These encoders are designed to handle dirty categorical data, which would be perfect for our use-case here. service = Service ( indexer = PyNNDescentIndexer ( metric = \"euclidean\" , n_jobs = 10 ), encoder = GapEncoder () ) service . train_from_dataf ( df ) Note that in this example, we've not specified any features= -parameters in our service.train_from_dataf call. In this case the service will assume all columns in the dataframe are relevant to encode. Query \u00b6 If we now want to construct a query, we will need to use all columns that we've encoded in our query call. That means that we need to query with a \"name\" , \"suburb\" and \"postcode\" keyword argument. service . query ( name = \"khimerc thmas\" , suburb = \"chariotte\" , postcode = \"28273\" , n_neighbors = 10 , out = \"dataframe\" ) This is the dataframe that we get out. name suburb postcode dist chimerc thmas chaflotte 28269 3.43277 quianna pope charlotte 28213 3.65635 chimerc thomas charlotte 28269 3.93795 khimerc thomas charlotte 2826g 3.99429 quianha pope charlotre 28213 5.47086 kendel beachum charlotte 28226 6.22856 mariq simpsony charlotte 28269 6.36486 quiarina pope charlotte 28113 6.57162 andrean polchow waxhaw 28173 7.21047 maria simpson charlotte 28269 8.2501 It certainly seems like the first few rows may indeed contain duplicates that we're interested in detecting. It deserves mentioning, once again, that the quality of our retreival depends a lot on our choice of index and encoding. But experimenting with this is exactly what this library makes easy. HTTP \u00b6 As always, you can easily turn this service into an API. service . serve ( host = '0.0.0.0' , port = 8080 ) But also here you'd need to mind the parameters that you send to the server. They need to correspond with the column names in the dataframe. This would be an appropriate payload for https://0.0.0.0:8080/query . { \"query\": { \"name\": \"khimerc thmas\", \"suburb\": \"chariotte\", \"postcode\": \"28273 }, \"n_neighbors\": 5 }","title":"Deduplication"},{"location":"quickstart/dedup/#example","text":"Let's consider the voters dataset. from simsity.datasets import fetch_voters df = fetch_voters () name suburb postcode khimerc thomas charlotte 2826g lucille richardst kannapolis 28o81 reb3cca bauerboand raleigh 27615 maleda mccloud goldsboro 2753o belida stovall morrisvill 27560 This dataset contains information about \"voters\" and the concern is that some of these rows may represent the same person. The persons name might occur in different spellings and the postcodes may contain typos, but they could still refer to the same person. In other words; there may be duplicates in this dataframe that we cannot remove with .drop_duplicates() . So how might we go about finding these?","title":"Example"},{"location":"quickstart/dedup/#similarity-service","text":"Let's build a similarity service, but now we'll use encoders from the dirty_cat package. These encoders are designed to handle dirty categorical data, which would be perfect for our use-case here. service = Service ( indexer = PyNNDescentIndexer ( metric = \"euclidean\" , n_jobs = 10 ), encoder = GapEncoder () ) service . train_from_dataf ( df ) Note that in this example, we've not specified any features= -parameters in our service.train_from_dataf call. In this case the service will assume all columns in the dataframe are relevant to encode.","title":"Similarity Service!"},{"location":"quickstart/dedup/#query","text":"If we now want to construct a query, we will need to use all columns that we've encoded in our query call. That means that we need to query with a \"name\" , \"suburb\" and \"postcode\" keyword argument. service . query ( name = \"khimerc thmas\" , suburb = \"chariotte\" , postcode = \"28273\" , n_neighbors = 10 , out = \"dataframe\" ) This is the dataframe that we get out. name suburb postcode dist chimerc thmas chaflotte 28269 3.43277 quianna pope charlotte 28213 3.65635 chimerc thomas charlotte 28269 3.93795 khimerc thomas charlotte 2826g 3.99429 quianha pope charlotre 28213 5.47086 kendel beachum charlotte 28226 6.22856 mariq simpsony charlotte 28269 6.36486 quiarina pope charlotte 28113 6.57162 andrean polchow waxhaw 28173 7.21047 maria simpson charlotte 28269 8.2501 It certainly seems like the first few rows may indeed contain duplicates that we're interested in detecting. It deserves mentioning, once again, that the quality of our retreival depends a lot on our choice of index and encoding. But experimenting with this is exactly what this library makes easy.","title":"Query"},{"location":"quickstart/dedup/#http","text":"As always, you can easily turn this service into an API. service . serve ( host = '0.0.0.0' , port = 8080 ) But also here you'd need to mind the parameters that you send to the server. They need to correspond with the column names in the dataframe. This would be an appropriate payload for https://0.0.0.0:8080/query . { \"query\": { \"name\": \"khimerc thmas\", \"suburb\": \"chariotte\", \"postcode\": \"28273 }, \"n_neighbors\": 5 }","title":"HTTP"}]}